{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pathlib import Path \n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random \n",
    "\n",
    "# fastai\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *\n",
    "\n",
    "# transformers\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersBaseTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n",
    "        self._pretrained_tokenizer = pretrained_tokenizer\n",
    "        self.max_seq_len = pretrained_tokenizer.max_len\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __call__(self, *args, **kwargs): \n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
    "        CLS = self._pretrained_tokenizer.cls_token\n",
    "        SEP = self._pretrained_tokenizer.sep_token\n",
    "        if self.model_type in ['roberta']:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
    "            tokens = [CLS] + tokens + [SEP]\n",
    "        else:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
    "            if self.model_type in ['xlnet']:\n",
    "                tokens = tokens + [SEP] +  [CLS]\n",
    "            else:\n",
    "                tokens = [CLS] + tokens + [SEP]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersVocab(Vocab):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        super(TransformersVocab, self).__init__(itos = [])\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
    "        \"Convert a list of tokens `t` to their ids.\"\n",
    "        return self.tokenizer.convert_tokens_to_ids(t)\n",
    "        #return self.tokenizer.encode(t)\n",
    "\n",
    "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
    "        \"Convert a list of `nums` to their tokens.\"\n",
    "        nums = np.array(nums).tolist()\n",
    "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n",
    "\n",
    "    def __setstate__(self, state:dict):\n",
    "        self.itos = state['itos']\n",
    "        self.tokenizer = state['tokenizer']\n",
    "        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our model architecture \n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, transformer_model: PreTrainedModel):\n",
    "        super(CustomTransformerModel,self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \n",
    "        # attention_mask\n",
    "        # Mask to avoid performing attention on padding token indices.\n",
    "        # Mask values selected in ``[0, 1]``:\n",
    "        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
    "        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n",
    "        \n",
    "        logits = self.transformer(input_ids,\n",
    "                                  attention_mask = attention_mask)[0]   \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'roberta'\n",
    "pretrained_model_name = 'roberta-base'\n",
    "\n",
    "model_class, tokenizer_class, config_class = RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
    "\n",
    "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
    "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
    "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])\n",
    "\n",
    "pad_idx = transformer_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "  sentiment = learner.predict(text)[1].item()\n",
    "  return sentiment\n",
    "\n",
    "def sentiment_label (Sentiment):\n",
    "   if Sentiment == 2:\n",
    "       return \"positive\"\n",
    "   elif Sentiment == 0:\n",
    "       return \"negative\"\n",
    "   else :\n",
    "       return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_ROOT = Path(\"..\") / '/content/drive/My Drive/Final_Datasets/Test_India.csv'\n",
    "predictions_test = pd.read_csv(DATA_ROOT)\n",
    "\n",
    "predictions_test['Prediction'] = predictions_test['full_text'].apply(predict_sentiment)\n",
    "predictions_test['Prediction_text'] = predictions_test['Prediction'].apply(sentiment_label)\n",
    "class_names = ['negative','positive','neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predictions_test['Sentiment'], predictions_test['Prediction_text'], target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(predictions_test['Sentiment'],predictions_test['Prediction_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Confusion matrix using Seaborn Library\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cf,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-Time Twitter Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining functions for Cleaning and Normalization of Data\n",
    "import re\n",
    "import string\n",
    "def replace_url(string): # cleaning of URL\n",
    "    text = re.sub(r'http\\S+', 'LINK', string)\n",
    "    return text\n",
    "\n",
    "\n",
    "def replace_email(text):#Cleaning of Email related text\n",
    "    line = re.sub(r'[\\w\\.-]+@[\\w\\.-]+','MAIL',str(text))\n",
    "    return \"\".join(line)\n",
    "\n",
    "def rep(text):#cleaning of non standard words\n",
    "    grp = text.group(0)\n",
    "    if len(grp) > 3:\n",
    "        return grp[0:2]\n",
    "    else:\n",
    "        return grp# can change the value here on repetition\n",
    "def unique_char(rep,sentence):\n",
    "    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n",
    "    return convert\n",
    "\n",
    "def find_dollar(text):#Finding the dollar sign in the text\n",
    "    line=re.sub(r'\\$\\d+(?:\\.\\d+)?','PRICE',text)\n",
    "    return \"\".join(line)\n",
    "\n",
    "def replace_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'EMOJI', text) \n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n",
    "          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n",
    "          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n",
    "          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n",
    "          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n",
    "          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n",
    "          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text)\n",
    "    for punct in puncts + list(string.punctuation):\n",
    "        if punct in text:\n",
    "            text = text.replace(punct, f'')\n",
    "    return text\n",
    "   \n",
    "def replace_asterisk(text):\n",
    "    text = re.sub(\"\\*\", 'ABUSE ', text)\n",
    "    return text\n",
    "\n",
    "def remove_duplicates(text):\n",
    "    text = re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', text)\n",
    "    return text\n",
    "\n",
    "def change(text):\n",
    "    if(text == ''):\n",
    "        return text\n",
    "  #calling the subfunctions in the cleaning function\n",
    "    text = replace_email(text)\n",
    "    text = replace_url(text)\n",
    "    text = unique_char(rep,text)\n",
    "    text = replace_asterisk(text)\n",
    "    text = remove_duplicates(text)\n",
    "    text = clean_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Import libraries\n",
    "from tweepy import OAuthHandler\n",
    "#from tweepy.streaming import StreamListener\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "#import re\n",
    "#from textblob import TextBlob\n",
    "#import string\n",
    "#import preprocessor as p\n",
    "#import os\n",
    "import time\n",
    "\n",
    "# Twitter credentials\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_key = ''\n",
    "access_secret = ''\n",
    "\n",
    "# Pass your twitter credentials to tweepy via its OAuthHandler\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "def extract_tweets(search_words,date_since,numTweets):\n",
    "  return(tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraptweets(search_words, date_since, numTweets, numRuns):\n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following', 'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts', 'retweetcount', 'text', 'hashtags'])\n",
    "    #db_tweets = pd.DataFrame()\n",
    "\n",
    "    for i in range(numRuns):\n",
    "\n",
    "        tweets = extract_tweets(search_words,date_since,numTweets)\n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        print(len(tweet_list))\n",
    "        noTweets = 0\n",
    "\n",
    "        for tweet in tweet_list:\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "            lst=[]\n",
    "            for h in hashtags:\n",
    "                lst.append(h['text'])\n",
    "            try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                text = tweet.full_text\n",
    "\n",
    "            itweet = [username,acctdesc,location,following,followers,totaltweets,usercreatedts,tweetcreatedts,retweetcount,text,lst]\n",
    "            db_tweets.loc[len(db_tweets)] = itweet\n",
    "\n",
    "            noTweets += 1\n",
    "            print(noTweets)\n",
    "\n",
    "            #filename = \"tweets.csv\"\n",
    "            #with open(filename, \"a\", newline='') as fp:\n",
    "             #   wr = csv.writer(fp, dialect='excel')\n",
    "              #  wr.writerow(itweet)\n",
    "\n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        if i+1 != numRuns:\n",
    "            time.sleep(920)\n",
    "\n",
    "        filename = \"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/tweets.csv\"\n",
    "        db_tweets['text'] = db_tweets['text'].apply(change)\n",
    "        db_tweets = db_tweets[['tweetcreatedts', 'retweetcount', 'text']]\n",
    "        # Store dataframe in csv with creation date timestamp\n",
    "        db_tweets.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise these variables:\n",
    "\n",
    "search_words = \"#India AND #COVID-19\"\n",
    "date_since = \"2020-04-29\"\n",
    "#date_until = \"2020-05-01\"\n",
    "numTweets = 2500\n",
    "numRuns = 1\n",
    "# Call the function scraptweets\n",
    "program_start = time.time()\n",
    "scraptweets(search_words, date_since, numTweets, numRuns)\n",
    "program_end = time.time()\n",
    "print('Scraping has completed!')\n",
    "print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-Time Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/content/drive/My Drive/Real-time-Data/tweets.csv'\n",
    "predictions = pd.read_csv(path)\n",
    "\n",
    "predictions['Prediction'] = predictions['text'].apply(predict_sentiment)\n",
    "predictions['Prediction'] = predictions['Prediction'].apply(sentiment_label)\n",
    "class_names = ['negative','positive','neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.rename(columns={'Prediction':'sentiment'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.drop(['tweetcreatedts'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
